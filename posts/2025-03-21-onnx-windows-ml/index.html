<!doctype html><html lang=es><head><title>Integraci√≥n de inferencia local ONNX con Windows ML en apps modernas ¬∑ Christian Amado
</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Christian Amado"><meta name=description content="La inteligencia artificial en el entorno de escritorio ya no est√° limitada a la nube. Windows proporciona soporte nativo para la ejecuci√≥n de modelos ONNX directamente desde una app WinUI 3 utilizando Windows ML. Esto permite realizar inferencia local con alto rendimiento y sin necesidad de conexi√≥n a internet, ideal para escenarios de privacidad, baja latencia o ejecuci√≥n offline.
En este art√≠culo se describe c√≥mo integrar un modelo ONNX a una app moderna, c√≥mo usar la API de Windows ML y c√≥mo estructurar un flujo de inferencia optimizado con soporte para entrada y salida de datos reales."><meta name=keywords content="blog,desarrollador,personal"><meta name=twitter:card content="summary"><meta name=twitter:title content="Integraci√≥n de inferencia local ONNX con Windows ML en apps modernas"><meta name=twitter:description content="La inteligencia artificial en el entorno de escritorio ya no est√° limitada a la nube. Windows proporciona soporte nativo para la ejecuci√≥n de modelos ONNX directamente desde una app WinUI 3 utilizando Windows ML. Esto permite realizar inferencia local con alto rendimiento y sin necesidad de conexi√≥n a internet, ideal para escenarios de privacidad, baja latencia o ejecuci√≥n offline.
En este art√≠culo se describe c√≥mo integrar un modelo ONNX a una app moderna, c√≥mo usar la API de Windows ML y c√≥mo estructurar un flujo de inferencia optimizado con soporte para entrada y salida de datos reales."><meta property="og:url" content="https://cmas.dev/posts/2025-03-21-onnx-windows-ml/"><meta property="og:site_name" content="Christian Amado"><meta property="og:title" content="Integraci√≥n de inferencia local ONNX con Windows ML en apps modernas"><meta property="og:description" content="La inteligencia artificial en el entorno de escritorio ya no est√° limitada a la nube. Windows proporciona soporte nativo para la ejecuci√≥n de modelos ONNX directamente desde una app WinUI 3 utilizando Windows ML. Esto permite realizar inferencia local con alto rendimiento y sin necesidad de conexi√≥n a internet, ideal para escenarios de privacidad, baja latencia o ejecuci√≥n offline.
En este art√≠culo se describe c√≥mo integrar un modelo ONNX a una app moderna, c√≥mo usar la API de Windows ML y c√≥mo estructurar un flujo de inferencia optimizado con soporte para entrada y salida de datos reales."><meta property="og:locale" content="es"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-03-21T00:00:00-04:00"><meta property="article:modified_time" content="2025-03-21T00:00:00-04:00"><meta property="article:tag" content="WinDev"><meta property="article:tag" content="Windows 11"><meta property="article:tag" content="WinUI 3"><meta property="article:tag" content="Windows App SDK"><script async src="https://www.googletagmanager.com/gtag/js?id=G-V1ZRP82YFD"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-V1ZRP82YFD")</script><link rel=canonical href=https://cmas.dev/posts/2025-03-21-onnx-windows-ml/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.7763f8bc6341ecf82378e867c285e1549abb063a899be313ccd25dbfcd24fa7d.css integrity="sha256-d2P4vGNB7PgjeOhnwoXhVJq7BjqJm+MTzNJdv80k+n0=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=apple-touch-icon sizes=57x57 href=/img/apple-icon-57x57.png><link rel=apple-touch-icon sizes=60x60 href=/img/apple-icon-60x60.png><link rel=apple-touch-icon sizes=72x72 href=/img/apple-icon-72x72.png><link rel=apple-touch-icon sizes=76x76 href=/img/apple-icon-76x76.png><link rel=apple-touch-icon sizes=114x114 href=/img/apple-icon-114x114.png><link rel=apple-touch-icon sizes=120x120 href=/img/apple-icon-120x120.png><link rel=apple-touch-icon sizes=144x144 href=/img/apple-icon-144x144.png><link rel=apple-touch-icon sizes=152x152 href=/img/apple-icon-152x152.png><link rel=apple-touch-icon sizes=180x180 href=/img/apple-icon-180x180.png><link rel=icon type=image/png sizes=192x192 href=/img/android-icon-192x192.png><link rel=icon type=image/png sizes=32x32 href=/img/favicon-32x32.png><link rel=icon type=image/png sizes=96x96 href=/img/favicon-96x96.png><link rel=icon type=image/png sizes=16x16 href=/img/favicon-16x16.png><link rel=manifest href=/img/manifest.json><meta name=msapplication-TileColor content="#ffffff"><meta name=msapplication-TileImage content="/img/ms-icon-144x144.png"><meta name=theme-color content="#ffffff"><link rel=mask-icon href=/img/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://cmas.dev/>Christian Amado
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/posts/>Blog</a></li><li class=navigation-item><a class=navigation-link href=/about>Biograf√≠a</a></li><li class=navigation-item><a class=navigation-link href=/contact>Contacto</a></li><li class=navigation-item><a class=navigation-link href=/diabetes>Diabetes</a></li><li class=navigation-item><a class=navigation-link href=/tags>Etiquetas</a></li><li class="navigation-item menu-separator"><span>|</span></li><li class=navigation-item><a href=/en/>üá∫üá∏</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://cmas.dev/posts/2025-03-21-onnx-windows-ml/>Integraci√≥n de inferencia local ONNX con Windows ML en apps modernas</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa-solid fa-calendar" aria-hidden=true></i>
<time datetime=2025-03-21T00:00:00-04:00>marzo 21, 2025
</time></span><span class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>
3 minutos de lectura.</span></div><div class=tags><i class="fa-solid fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/windev/>WinDev</a>
</span><span class=separator>‚Ä¢</span>
<span class=tag><a href=/tags/windows-11/>Windows 11</a>
</span><span class=separator>‚Ä¢</span>
<span class=tag><a href=/tags/winui-3/>WinUI 3</a>
</span><span class=separator>‚Ä¢</span>
<span class=tag><a href=/tags/windows-app-sdk/>Windows App SDK</a></span></div></div></header><div class=post-content><p>La inteligencia artificial en el entorno de escritorio ya no est√° limitada a la nube. <strong>Windows</strong> proporciona soporte nativo para la ejecuci√≥n de modelos <strong>ONNX</strong> directamente desde una app <strong>WinUI 3</strong> utilizando <strong>Windows ML</strong>. Esto permite realizar inferencia local con alto rendimiento y sin necesidad de conexi√≥n a internet, ideal para escenarios de privacidad, baja latencia o ejecuci√≥n offline.</p><p>En este art√≠culo se describe c√≥mo integrar un modelo <strong>ONNX</strong> a una app moderna, c√≥mo usar la <strong>API</strong> de <strong>Windows ML</strong> y c√≥mo estructurar un flujo de inferencia optimizado con soporte para entrada y salida de datos reales.</p><h2 id=requisitos>Requisitos
<a class=heading-link href=#requisitos><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><ul><li>Windows 11 (recomendado build 22621+)</li><li>Visual Studio 2022 con Windows App SDK 1.3+</li><li>Proyecto WinUI 3 empaquetado (MSIX)</li><li>Referencia al paquete <code>Microsoft.AI.MachineLearning</code></li><li>Modelo ONNX compatible (puede ser descargado de onnxruntime zoo)</li></ul><h2 id=paso-1-agregar-el-paquete-nuget-de-windows-ml>Paso 1: Agregar el paquete NuGet de Windows ML
<a class=heading-link href=#paso-1-agregar-el-paquete-nuget-de-windows-ml><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Agregar al proyecto:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Install-Package Microsoft.AI.MachineLearning
</span></span></code></pre></div><p>Esto permite cargar y ejecutar modelos ONNX directamente desde c√≥digo.</p><h2 id=paso-2-copiar-el-modelo-onnx-al-proyecto>Paso 2: Copiar el modelo ONNX al proyecto
<a class=heading-link href=#paso-2-copiar-el-modelo-onnx-al-proyecto><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Ejemplo: <code>modelo_mnist.onnx</code> copiado a la carpeta <code>Assets/ML</code></p><p>Configurar en propiedades:</p><ul><li>Build Action: Content</li><li>Copy to Output Directory: Copy if newer</li></ul><h2 id=paso-3-cargar-el-modelo-onnx>Paso 3: Cargar el modelo ONNX
<a class=heading-link href=#paso-3-cargar-el-modelo-onnx><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Importar:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-csharp data-lang=csharp><span class=line><span class=cl><span class=k>using</span> <span class=nn>Microsoft.AI.MachineLearning</span><span class=p>;</span>
</span></span></code></pre></div><p>Cargar desde archivo:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-csharp data-lang=csharp><span class=line><span class=cl><span class=kt>var</span> <span class=n>file</span> <span class=p>=</span> <span class=k>await</span> <span class=n>StorageFile</span><span class=p>.</span><span class=n>GetFileFromApplicationUriAsync</span><span class=p>(</span><span class=k>new</span> <span class=n>Uri</span><span class=p>(</span><span class=s>&#34;ms-appx:///Assets/ML/modelo_mnist.onnx&#34;</span><span class=p>));</span>
</span></span><span class=line><span class=cl><span class=kt>var</span> <span class=n>session</span> <span class=p>=</span> <span class=n>LearningModelSession</span><span class=p>.</span><span class=n>CreateFromModel</span><span class=p>(</span><span class=n>LearningModel</span><span class=p>.</span><span class=n>LoadFromFilePath</span><span class=p>(</span><span class=n>file</span><span class=p>.</span><span class=n>Path</span><span class=p>));</span>
</span></span></code></pre></div><p>Esto crea una sesi√≥n de inferencia desde el modelo.</p><h2 id=paso-4-preparar-entrada-para-inferencia>Paso 4: Preparar entrada para inferencia
<a class=heading-link href=#paso-4-preparar-entrada-para-inferencia><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Ejemplo para imagen de 28x28 p√≠xeles escala de grises:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-csharp data-lang=csharp><span class=line><span class=cl><span class=kt>float</span><span class=p>[]</span> <span class=n>input</span> <span class=p>=</span> <span class=k>new</span> <span class=kt>float</span><span class=p>[</span><span class=m>1</span> <span class=p>*</span> <span class=m>1</span> <span class=p>*</span> <span class=m>28</span> <span class=p>*</span> <span class=m>28</span><span class=p>];</span> <span class=c1>// NCHW</span>
</span></span><span class=line><span class=cl><span class=c1>// Rellenar input con los valores de p√≠xeles normalizados (0.0‚Äì1.0)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kt>var</span> <span class=n>tensor</span> <span class=p>=</span> <span class=n>TensorFloat</span><span class=p>.</span><span class=n>CreateFromArray</span><span class=p>(</span><span class=k>new</span><span class=p>[]</span> <span class=p>{</span> <span class=m>1</span><span class=n>u</span><span class=p>,</span> <span class=m>1</span><span class=n>u</span><span class=p>,</span> <span class=m>28</span><span class=n>u</span><span class=p>,</span> <span class=m>28</span><span class=n>u</span> <span class=p>},</span> <span class=n>input</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=kt>var</span> <span class=n>binding</span> <span class=p>=</span> <span class=k>new</span> <span class=n>LearningModelBinding</span><span class=p>(</span><span class=n>session</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=n>binding</span><span class=p>.</span><span class=n>Bind</span><span class=p>(</span><span class=s>&#34;Input3&#34;</span><span class=p>,</span> <span class=n>tensor</span><span class=p>);</span>
</span></span></code></pre></div><h2 id=paso-5-ejecutar-inferencia>Paso 5: Ejecutar inferencia
<a class=heading-link href=#paso-5-ejecutar-inferencia><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-csharp data-lang=csharp><span class=line><span class=cl><span class=kt>var</span> <span class=n>result</span> <span class=p>=</span> <span class=k>await</span> <span class=n>session</span><span class=p>.</span><span class=n>EvaluateAsync</span><span class=p>(</span><span class=n>binding</span><span class=p>,</span> <span class=s>&#34;Inferencia1&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=kt>var</span> <span class=n>outputTensor</span> <span class=p>=</span> <span class=n>result</span><span class=p>.</span><span class=n>Outputs</span><span class=p>[</span><span class=s>&#34;Plus214_Output_0&#34;</span><span class=p>]</span> <span class=k>as</span> <span class=n>TensorFloat</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=kt>var</span> <span class=n>output</span> <span class=p>=</span> <span class=n>outputTensor</span><span class=p>.</span><span class=n>GetAsVectorView</span><span class=p>();</span>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=n>predictedIndex</span> <span class=p>=</span> <span class=n>output</span><span class=p>.</span><span class=n>ToList</span><span class=p>().</span><span class=n>IndexOf</span><span class=p>(</span><span class=n>output</span><span class=p>.</span><span class=n>Max</span><span class=p>());</span>
</span></span></code></pre></div><p>Esto retorna el √≠ndice con mayor probabilidad (en este caso, el d√≠gito 0‚Äì9).</p><h2 id=paso-6-mostrar-resultados-en-ui>Paso 6: Mostrar resultados en UI
<a class=heading-link href=#paso-6-mostrar-resultados-en-ui><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-csharp data-lang=csharp><span class=line><span class=cl><span class=n>ResultadoText</span><span class=p>.</span><span class=n>Text</span> <span class=p>=</span> <span class=s>$&#34;Predicci√≥n: {predictedIndex}&#34;</span><span class=p>;</span>
</span></span></code></pre></div><p>En XAML:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-xml data-lang=xml><span class=line><span class=cl><span class=nt>&lt;StackPanel&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nt>&lt;Image</span> <span class=na>x:Name=</span><span class=s>&#34;InputPreview&#34;</span><span class=nt>/&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nt>&lt;TextBlock</span> <span class=na>x:Name=</span><span class=s>&#34;ResultadoText&#34;</span><span class=nt>/&gt;</span>
</span></span><span class=line><span class=cl><span class=nt>&lt;/StackPanel&gt;</span>
</span></span></code></pre></div><h2 id=paso-7-optimizaci√≥n-y-aceleraci√≥n>Paso 7: Optimizaci√≥n y aceleraci√≥n
<a class=heading-link href=#paso-7-optimizaci%c3%b3n-y-aceleraci%c3%b3n><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Windows ML usa DirectML y soporte para GPU si est√° disponible. No se requiere configuraci√≥n extra: el runtime lo detecta autom√°ticamente.</p><p>Validar el dispositivo utilizado:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-csharp data-lang=csharp><span class=line><span class=cl><span class=kt>var</span> <span class=n>deviceKind</span> <span class=p>=</span> <span class=n>session</span><span class=p>.</span><span class=n>DeviceKind</span><span class=p>.</span><span class=n>ToString</span><span class=p>();</span> <span class=c1>// CPU, GPU, DirectML</span>
</span></span></code></pre></div><h2 id=paso-8-alternativas-para-modelos>Paso 8: Alternativas para modelos
<a class=heading-link href=#paso-8-alternativas-para-modelos><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><ul><li>Convertir PyTorch / TensorFlow ‚Üí ONNX</li><li>Usar modelos preentrenados: <code>squeezenet</code>, <code>resnet18</code>, <code>mobilenetv2</code></li><li>Exportar modelos propios con herramientas como <code>onnxruntime-tools</code> o <code>tf2onnx</code></li></ul><h2 id=casos-de-uso-reales>Casos de uso reales
<a class=heading-link href=#casos-de-uso-reales><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><ul><li>Clasificaci√≥n de im√°genes (productos, documentos)</li><li>Detecci√≥n de anomal√≠as locales</li><li>Asistentes inteligentes offline</li><li>Modelos de NLP ligeros ejecutados localmente</li><li>Reconocimiento de escritura a mano o n√∫meros</li></ul><h2 id=buenas-pr√°cticas>Buenas pr√°cticas
<a class=heading-link href=#buenas-pr%c3%a1cticas><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><ul><li>Validar entradas antes de ejecutar inferencia</li><li>Manejar fallback si no hay aceleraci√≥n GPU</li><li>No cargar el modelo en cada inferencia (persistir sesi√≥n)</li><li>Reducir tama√±o del modelo con quantization si es posible</li></ul><h2 id=conclusi√≥n>Conclusi√≥n
<a class=heading-link href=#conclusi%c3%b3n><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>La integraci√≥n de <strong>ONNX</strong> y <strong>Windows ML</strong> en aplicaciones modernas <strong>WinUI 3</strong> permite ejecutar inferencia local de modelos de IA de forma nativa, segura y sin latencia de red. Esto convierte a <strong>Windows</strong> en una plataforma ideal para aplicaciones inteligentes en el borde, desde escritorios empresariales hasta dispositivos aut√≥nomos. El soporte de <strong>Windows App SDK</strong> lo hace accesible a cualquier desarrollador de apps modernas.</p></div><footer><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//cmasblog.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}(),document.addEventListener("themeChanged",function(){document.readyState=="complete"&&DISQUS.reset({reload:!0,config:disqus_config})})</script></footer></article></section></div><footer class=footer><section class=container>¬©
2018 -
2025
Christian Amado
¬∑
Desarrollado por <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script><script async src="https://www.googletagmanager.com/gtag/js?id=356375808"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","356375808")}</script><script async src="https://www.googletagmanager.com/gtag/js?id=013131931956450466198%3ablhmkdpweyq"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","013131931956450466198:blhmkdpweyq")</script></body></html>
<!doctype html><html lang=es><head><title>Machine learning local distribuido con WSL2 + containers ¬∑ Christian Amado
</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Christian Amado"><meta name=description content="Una de las grandes ventajas de WSL2 es su compatibilidad con contenedores Docker y acceso a la GPU, lo que lo convierte en un entorno ideal para correr flujos de machine learning distribuido desde una √∫nica m√°quina con m√∫ltiples contenedores Linux. Esto permite simular ambientes de entrenamiento paralelos, microservicios de inferencia o arquitecturas de orquestaci√≥n como Ray o Dask desde el entorno Windows, sin necesidad de un cl√∫ster real o acceso a la nube.
Este art√≠culo describe c√≥mo configurar y ejecutar cargas distribuidas de ML usando Docker, PyTorch, TensorFlow, y Ray sobre WSL2, aprovechando la integraci√≥n con NVIDIA GPU y el rendimiento de Linux."><meta name=keywords content="blog,desarrollador,personal"><meta name=twitter:card content="summary"><meta name=twitter:title content="Machine learning local distribuido con WSL2 + containers"><meta name=twitter:description content="Una de las grandes ventajas de WSL2 es su compatibilidad con contenedores Docker y acceso a la GPU, lo que lo convierte en un entorno ideal para correr flujos de machine learning distribuido desde una √∫nica m√°quina con m√∫ltiples contenedores Linux. Esto permite simular ambientes de entrenamiento paralelos, microservicios de inferencia o arquitecturas de orquestaci√≥n como Ray o Dask desde el entorno Windows, sin necesidad de un cl√∫ster real o acceso a la nube.
Este art√≠culo describe c√≥mo configurar y ejecutar cargas distribuidas de ML usando Docker, PyTorch, TensorFlow, y Ray sobre WSL2, aprovechando la integraci√≥n con NVIDIA GPU y el rendimiento de Linux."><meta property="og:url" content="https://cmas.dev/posts/2024-10-25-wsl2-machine-learning/"><meta property="og:site_name" content="Christian Amado"><meta property="og:title" content="Machine learning local distribuido con WSL2 + containers"><meta property="og:description" content="Una de las grandes ventajas de WSL2 es su compatibilidad con contenedores Docker y acceso a la GPU, lo que lo convierte en un entorno ideal para correr flujos de machine learning distribuido desde una √∫nica m√°quina con m√∫ltiples contenedores Linux. Esto permite simular ambientes de entrenamiento paralelos, microservicios de inferencia o arquitecturas de orquestaci√≥n como Ray o Dask desde el entorno Windows, sin necesidad de un cl√∫ster real o acceso a la nube.
Este art√≠culo describe c√≥mo configurar y ejecutar cargas distribuidas de ML usando Docker, PyTorch, TensorFlow, y Ray sobre WSL2, aprovechando la integraci√≥n con NVIDIA GPU y el rendimiento de Linux."><meta property="og:locale" content="es"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-10-25T00:00:00-04:00"><meta property="article:modified_time" content="2024-10-25T00:00:00-04:00"><meta property="article:tag" content="WinDev"><meta property="article:tag" content="Windows 11"><meta property="article:tag" content="WSL"><script async src="https://www.googletagmanager.com/gtag/js?id=G-V1ZRP82YFD"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-V1ZRP82YFD")</script><link rel=canonical href=https://cmas.dev/posts/2024-10-25-wsl2-machine-learning/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.7763f8bc6341ecf82378e867c285e1549abb063a899be313ccd25dbfcd24fa7d.css integrity="sha256-d2P4vGNB7PgjeOhnwoXhVJq7BjqJm+MTzNJdv80k+n0=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=apple-touch-icon sizes=57x57 href=/img/apple-icon-57x57.png><link rel=apple-touch-icon sizes=60x60 href=/img/apple-icon-60x60.png><link rel=apple-touch-icon sizes=72x72 href=/img/apple-icon-72x72.png><link rel=apple-touch-icon sizes=76x76 href=/img/apple-icon-76x76.png><link rel=apple-touch-icon sizes=114x114 href=/img/apple-icon-114x114.png><link rel=apple-touch-icon sizes=120x120 href=/img/apple-icon-120x120.png><link rel=apple-touch-icon sizes=144x144 href=/img/apple-icon-144x144.png><link rel=apple-touch-icon sizes=152x152 href=/img/apple-icon-152x152.png><link rel=apple-touch-icon sizes=180x180 href=/img/apple-icon-180x180.png><link rel=icon type=image/png sizes=192x192 href=/img/android-icon-192x192.png><link rel=icon type=image/png sizes=32x32 href=/img/favicon-32x32.png><link rel=icon type=image/png sizes=96x96 href=/img/favicon-96x96.png><link rel=icon type=image/png sizes=16x16 href=/img/favicon-16x16.png><link rel=manifest href=/img/manifest.json><meta name=msapplication-TileColor content="#ffffff"><meta name=msapplication-TileImage content="/img/ms-icon-144x144.png"><meta name=theme-color content="#ffffff"><link rel=mask-icon href=/img/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://cmas.dev/>Christian Amado
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/posts/>Blog</a></li><li class=navigation-item><a class=navigation-link href=/about>Biograf√≠a</a></li><li class=navigation-item><a class=navigation-link href=/contact>Contacto</a></li><li class=navigation-item><a class=navigation-link href=/diabetes>Diabetes</a></li><li class=navigation-item><a class=navigation-link href=/tags>Etiquetas</a></li><li class="navigation-item menu-separator"><span>|</span></li><li class=navigation-item><a href=/en/>üá∫üá∏</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://cmas.dev/posts/2024-10-25-wsl2-machine-learning/>Machine learning local distribuido con WSL2 + containers</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa-solid fa-calendar" aria-hidden=true></i>
<time datetime=2024-10-25T00:00:00-04:00>octubre 25, 2024
</time></span><span class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>
3 minutos de lectura.</span></div><div class=tags><i class="fa-solid fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/windev/>WinDev</a>
</span><span class=separator>‚Ä¢</span>
<span class=tag><a href=/tags/windows-11/>Windows 11</a>
</span><span class=separator>‚Ä¢</span>
<span class=tag><a href=/tags/wsl/>WSL</a></span></div></div></header><div class=post-content><p>Una de las grandes ventajas de WSL2 es su compatibilidad con contenedores Docker y acceso a la GPU, lo que lo convierte en un entorno ideal para correr flujos de <strong>machine learning distribuido</strong> desde una √∫nica m√°quina con m√∫ltiples contenedores Linux. Esto permite simular ambientes de entrenamiento paralelos, microservicios de inferencia o arquitecturas de orquestaci√≥n como Ray o Dask desde el entorno Windows, sin necesidad de un cl√∫ster real o acceso a la nube.</p><p>Este art√≠culo describe c√≥mo configurar y ejecutar cargas distribuidas de ML usando Docker, PyTorch, TensorFlow, y Ray sobre WSL2, aprovechando la integraci√≥n con NVIDIA GPU y el rendimiento de Linux.</p><h2 id=requisitos>Requisitos
<a class=heading-link href=#requisitos><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><ul><li>Windows 11 con WSL2</li><li>Docker Desktop con soporte WSL2 y GPU habilitado</li><li>NVIDIA GPU compatible + drivers + WSL CUDA Toolkit</li><li>Distro Ubuntu en WSL2</li></ul><p>Verificar GPU:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>nvidia-smi
</span></span></code></pre></div><p>Verificar Docker:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>docker --version
</span></span><span class=line><span class=cl>docker info
</span></span></code></pre></div><h2 id=paso-1-crear-red-de-contenedores-para-ml-distribuido>Paso 1: Crear red de contenedores para ML distribuido
<a class=heading-link href=#paso-1-crear-red-de-contenedores-para-ml-distribuido><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>docker network create ml-network
</span></span></code></pre></div><p>Esto permite que contenedores se comuniquen por nombre dentro de la red.</p><h2 id=paso-2-crear-imagen-base-de-entrenamiento-con-gpu>Paso 2: Crear imagen base de entrenamiento con GPU
<a class=heading-link href=#paso-2-crear-imagen-base-de-entrenamiento-con-gpu><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Archivo <code>Dockerfile</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-Dockerfile data-lang=Dockerfile><span class=line><span class=cl><span class=k>FROM</span><span class=s> pytorch/pytorch:2.1.0-cuda11.8-cudnn8-runtime</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> apt update <span class=o>&amp;&amp;</span> apt install -y git iputils-ping<span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> pip install ray<span class=o>[</span>default<span class=o>]</span> dask scikit-learn pandas matplotlib<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>WORKDIR</span><span class=s> /workspace</span><span class=err>
</span></span></span></code></pre></div><p>Construir imagen:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>docker build -t ml-node .
</span></span></code></pre></div><h2 id=paso-3-ejecutar-contenedores-coordinados-ray>Paso 3: Ejecutar contenedores coordinados (Ray)
<a class=heading-link href=#paso-3-ejecutar-contenedores-coordinados-ray><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Iniciar nodo principal:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>docker run -it --rm --gpus all --network ml-network --name ray-head ml-node   ray start --head --port<span class=o>=</span><span class=m>6379</span>
</span></span></code></pre></div><p>En otra terminal, iniciar un nodo worker:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>docker run -it --rm --gpus all --network ml-network --name ray-worker ml-node   ray start --address<span class=o>=</span>ray-head:6379
</span></span></code></pre></div><p>Validar desde un contenedor:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>ray</span>
</span></span><span class=line><span class=cl><span class=n>ray</span><span class=o>.</span><span class=n>init</span><span class=p>(</span><span class=n>address</span><span class=o>=</span><span class=s1>&#39;auto&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>ray</span><span class=o>.</span><span class=n>cluster_resources</span><span class=p>())</span>
</span></span></code></pre></div><p>Esto habilita paralelismo local real, √∫til para prototipos, testing o benchmarking.</p><h2 id=paso-4-alternativa-con-dask>Paso 4: Alternativa con Dask
<a class=heading-link href=#paso-4-alternativa-con-dask><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Dask permite flujos similares con programaci√≥n paralela y pandas-like API.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>dask.distributed</span> <span class=kn>import</span> <span class=n>Client</span>
</span></span><span class=line><span class=cl><span class=n>client</span> <span class=o>=</span> <span class=n>Client</span><span class=p>(</span><span class=s2>&#34;scheduler:8786&#34;</span><span class=p>)</span>
</span></span></code></pre></div><h2 id=paso-5-entrenamiento-distribuido-con-pytorch-o-tensorflow>Paso 5: Entrenamiento distribuido con PyTorch o TensorFlow
<a class=heading-link href=#paso-5-entrenamiento-distribuido-con-pytorch-o-tensorflow><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Utilizar <code>torch.distributed</code> o <code>tf.distribute</code> para paralelismo manual o con librer√≠as de alto nivel.</p><p>Ejemplo b√°sico con <code>torch.multiprocessing</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.multiprocessing</span> <span class=k>as</span> <span class=nn>mp</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>train</span><span class=p>(</span><span class=n>rank</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Proceso </span><span class=si>{</span><span class=n>rank</span><span class=si>}</span><span class=s2> entrenando&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s2>&#34;__main__&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>mp</span><span class=o>.</span><span class=n>spawn</span><span class=p>(</span><span class=n>train</span><span class=p>,</span> <span class=n>nprocs</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span></code></pre></div><p>Correr en un contenedor con m√∫ltiples procesos o GPUs seg√∫n configuraci√≥n.</p><h2 id=casos-de-uso-pr√°cticos>Casos de uso pr√°cticos
<a class=heading-link href=#casos-de-uso-pr%c3%a1cticos><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><ul><li>Simulaci√≥n de entrenamiento distribuido sin necesidad de Kubernetes</li><li>Inferencia paralela desde varios servicios contenedorizados</li><li>Microservicios de ML (FastAPI + modelo) en red local entre contenedores</li><li>Pruebas de carga con frameworks de orquestaci√≥n y m√©tricas</li></ul><h2 id=buenas-pr√°cticas>Buenas pr√°cticas
<a class=heading-link href=#buenas-pr%c3%a1cticas><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><ul><li>Usar <code>docker-compose</code> para definir m√∫ltiples nodos y servicios</li><li>Configurar vol√∫menes compartidos con datasets</li><li>Asegurar exposici√≥n de puertos entre servicios</li><li>Monitorizar con herramientas como Ray Dashboard (<code>--dashboard-host 0.0.0.0</code>)</li><li>Usar contenedores separados por rol (data ingestion, training, inference)</li></ul><h2 id=conclusi√≥n>Conclusi√≥n
<a class=heading-link href=#conclusi%c3%b3n><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>WSL2, combinado con Docker y acceso a GPU, permite ejecutar flujos avanzados de machine learning distribuido desde una sola m√°quina. Esta capacidad potencia a desarrolladores, cient√≠ficos de datos y equipos de investigaci√≥n para testear y escalar soluciones ML de forma local, r√°pida y segura antes de llevarlas a producci√≥n.</p></div><footer><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//cmasblog.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}(),document.addEventListener("themeChanged",function(){document.readyState=="complete"&&DISQUS.reset({reload:!0,config:disqus_config})})</script></footer></article></section></div><footer class=footer><section class=container>¬©
2018 -
2025
Christian Amado
¬∑
Desarrollado por <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script><script async src="https://www.googletagmanager.com/gtag/js?id=356375808"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","356375808")}</script><script async src="https://www.googletagmanager.com/gtag/js?id=013131931956450466198%3ablhmkdpweyq"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","013131931956450466198:blhmkdpweyq")</script></body></html>